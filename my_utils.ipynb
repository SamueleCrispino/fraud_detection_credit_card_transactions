{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2724a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\s.crispino\\AppData\\Local\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "\n",
    "# For preprocessing and modeling\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# For Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from lime import lime_tabular\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d70d16",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e13c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple function useful to make some cells idempotent\n",
    "def get_original_training_data(light=True):\n",
    "    train = pd.read_csv(\"fraudTrain.csv\").drop(['Unnamed: 0'], axis=1,)\n",
    "    test = pd.read_csv(\"fraudTest.csv\").drop(['Unnamed: 0'], axis=1,)\n",
    "    \n",
    "    if light:\n",
    "        start = 173332\n",
    "        size = 400000\n",
    "        end_train = start + int(size*0.85)\n",
    "        end_test = end_train +  int(size*0.15)\n",
    "        \n",
    "        train_set = train[start: end_train]\n",
    "        test_set =  train[end_train:end_test]\n",
    "        \n",
    "        print(\"train shape \", train_set.shape)\n",
    "        print(\"test shape \", test_set.shape)\n",
    "        \n",
    "        return train_set, test_set\n",
    "        \n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def create_fraud_prob_by_amt(df):\n",
    "\n",
    "    # Step 1: Define the custom bins for the 'amt' column, with dynamic upper bound\n",
    "    max_amt = df['amt'].max()  # Get the maximum amount available in the 'amt' column\n",
    "    bin_edges = [0, 700, 900, 1000, 1200, max_amt]  # Use max_amt as the upper bound for the last bin\n",
    "    bin_labels = ['0-700', '700-900', '900-1000', '1000-1200', f'1200-{max_amt}']\n",
    "\n",
    "    # Step 2: Create a new feature 'amt_range' that assigns each 'amt' to the defined bins\n",
    "    df['amt_range'] = pd.cut(df['amt'], bins=bin_edges, labels=bin_labels, right=False)\n",
    "\n",
    "    # Step 3: Add 'NaN' as a valid category to the 'amt_range' column\n",
    "    df['amt_range'] = df['amt_range'].cat.add_categories('NaN')\n",
    "\n",
    "    # Handle NaN values in amt_range\n",
    "    df['amt_range'] = df['amt_range'].fillna('NaN')  # Assign 'NaN' to missing values\n",
    "\n",
    "    # Step 3: Sort data by 'trans_date_trans_time' to ensure the time order\n",
    "    df = df.sort_values(by=['trans_date_trans_time'])\n",
    "\n",
    "    # Step 4: Initialize cumulative fraud statistics for each amt_range\n",
    "    df['fraud_prob_by_amt'] = 0.0  # Initialize fraud probability\n",
    "\n",
    "    # Step 5: Calculate time-dependent fraud probability for each transaction based on amt_range\n",
    "    cumulative_fraud = {label: 0 for label in bin_labels}  # Store cumulative fraud count for each amt_range\n",
    "    cumulative_total = {label: 0 for label in bin_labels}  # Store cumulative total count for each amt_range\n",
    "\n",
    "    # Use tqdm to add a progress bar\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Calculating fraud probabilities\", unit=\"row\"):\n",
    "        # Get the amt_range for the current transaction\n",
    "        amt_range = row['amt_range']\n",
    "\n",
    "        # If amt_range is 'NaN', you can choose to handle it differently (e.g., assign a default probability)\n",
    "        if amt_range == 'NaN':\n",
    "            df.at[idx, 'fraud_prob_by_amt'] = df['is_fraud'].mean()  # or set a custom value\n",
    "        else:\n",
    "            # Calculate the cumulative fraud probability for the current amt_range\n",
    "            if cumulative_total[amt_range] > 0:\n",
    "                df.at[idx, 'fraud_prob_by_amt'] = cumulative_fraud[amt_range] / cumulative_total[amt_range]\n",
    "            else:\n",
    "                # If no previous transactions in the range, use the global fraud rate\n",
    "                df.at[idx, 'fraud_prob_by_amt'] = df['is_fraud'].mean()\n",
    "\n",
    "            # Update cumulative counts for amt_range\n",
    "            cumulative_total[amt_range] += 1\n",
    "            cumulative_fraud[amt_range] += row['is_fraud']\n",
    "\n",
    "    # Step 6: Drop the support column 'amt_range' after the 'fraud_prob_by_amt' feature has been created\n",
    "    df.drop(columns=['amt_range'], inplace=True)\n",
    "    \n",
    "    # Now, df contains the 'fraud_prob_by_amt' feature, which is time-dependent\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_category_fraud_probability(data):\n",
    "\n",
    "    # Step 1: Ensure the data is sorted by time\n",
    "    # Assuming the data['trans_date_trans_time'] is already a datetime column\n",
    "    data = data.sort_values(by=['category', 'trans_date_trans_time'])\n",
    "\n",
    "    data['category_fraud_probability'] = 0.0\n",
    "\n",
    "    # Step 3: Calculate cumulative stats for each category\n",
    "    category_groups = data.groupby('category')\n",
    "\n",
    "    # Iterate through each category group\n",
    "    for category, group in tqdm(category_groups, desc=\"Processing categories\", total=len(category_groups)):\n",
    "        # Initialize cumulative counts\n",
    "        total_count = 0\n",
    "        fraud_count = 0\n",
    "\n",
    "        # Iterate through each transaction in the specific category group\n",
    "        for idx, row in group.iterrows():\n",
    "            # Assign cumulative probability for the current transaction\n",
    "            if total_count > 0:\n",
    "                data.at[idx, 'category_fraud_probability'] = fraud_count / total_count\n",
    "            else:\n",
    "                # Default to global fraud rate for the first transaction in the category\n",
    "                data.at[idx, 'category_fraud_probability'] = data['is_fraud'].mean()\n",
    "\n",
    "            # Update cumulative counts\n",
    "            total_count += 1\n",
    "            fraud_count += row['is_fraud']\n",
    "\n",
    "    # The feature 'category_fraud_probability' is now time-dependent and ready to use\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def create_merchant_fraud_probability(data):\n",
    "\n",
    "    # Step 1: Ensure the data is sorted by time\n",
    "    # data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time']) --> already done my dear, already done\n",
    "    data = data.sort_values(by=['merchant', 'trans_date_trans_time'])\n",
    "\n",
    "    # Step 2: Initialize cumulative fraud statistics\n",
    "    data['merchant_fraud_probability'] = 0.0\n",
    "\n",
    "    # Step 3: Calculate cumulative stats for each merchant\n",
    "    merchant_groups = data.groupby('merchant')\n",
    "\n",
    "    ## Iterates through each merchant group\n",
    "    for merchant, group in tqdm(merchant_groups, desc=\"Processing merchants\", total=len(merchant_groups)):\n",
    "        # Initialize cumulative counts\n",
    "        total_count = 0\n",
    "        fraud_count = 0\n",
    "\n",
    "        ## Iteartes through each transaction of a specific mechant group\n",
    "        for idx, row in group.iterrows():\n",
    "            # Assign cumulative probability for the current transaction\n",
    "            if total_count > 0:\n",
    "                data.at[idx, 'merchant_fraud_probability'] = fraud_count / total_count\n",
    "            else:\n",
    "                # Default to global fraud rate for the first transaction\n",
    "                ## global fraud rate provides a reasonable prior estimate for the likelihood of fraud \n",
    "                ### based on the overall (training) dataset\n",
    "                ### This can make the feature more consistent across merchants, \n",
    "                ### especially for new merchants with very few transactions.\n",
    "                ### INSTEAD:\n",
    "                ### If you set the first fraud probability to 0, \n",
    "                ### you’re essentially assuming that there’s no chance of fraud for the first transaction, \n",
    "                ### which could bias the model.\n",
    "                data.at[idx, 'merchant_fraud_probability'] = data['is_fraud'].mean()\n",
    "\n",
    "            # Update cumulative counts\n",
    "            total_count += 1\n",
    "            fraud_count += row['is_fraud']\n",
    "\n",
    "    # The feature 'merchant_fraud_probability' is now time-dependent and ready to use\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_hourly_fraud_probability(data):\n",
    "    # Step 1: Ensure the data is sorted by time\n",
    "    data = data.sort_values(by=['hour', 'trans_date_trans_time'])\n",
    "\n",
    "    # Step 2: Initialize cumulative fraud statistics\n",
    "    data['hourly_fraud_probability'] = 0.0\n",
    "\n",
    "    # Step 3: Calculate cumulative stats for each hour group\n",
    "    hour_groups = data.groupby('hour')\n",
    "\n",
    "    ## Iterate through each hour group\n",
    "    for hour, group in tqdm(hour_groups, desc=\"Processing hours\", total=len(hour_groups)):\n",
    "        # Initialize cumulative counts\n",
    "        total_count = 0\n",
    "        fraud_count = 0\n",
    "\n",
    "        ## Iterate through each transaction of a specific hour group\n",
    "        for idx, row in group.iterrows():\n",
    "            # Assign cumulative probability for the current transaction\n",
    "            if total_count > 0:\n",
    "                data.at[idx, 'hourly_fraud_probability'] = fraud_count / total_count\n",
    "            else:\n",
    "                # Default to global fraud rate for the first transaction in each group\n",
    "                data.at[idx, 'hourly_fraud_probability'] = data['is_fraud'].mean()\n",
    "\n",
    "            # Update cumulative counts\n",
    "            total_count += 1\n",
    "            fraud_count += row['is_fraud']\n",
    "\n",
    "    # The feature 'hourly_fraud_probability' is now time-dependent and ready to use\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Distance between transaction and merchant locations\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of Earth in kilometers\n",
    "    R = 6371\n",
    "    # Convert degrees to radians\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2)**2\n",
    "    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "\n",
    "def data_processing_updated(complete_df, historical=False):\n",
    "    \n",
    "\n",
    "    # Distance between transaction and merchant locations\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        # Radius of Earth in kilometers\n",
    "        R = 6371\n",
    "        # Convert degrees to radians\n",
    "        phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "        dphi = np.radians(lat2 - lat1)\n",
    "        dlambda = np.radians(lon2 - lon1)\n",
    "        a = np.sin(dphi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2)**2\n",
    "        return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    def not_aggreageted_metric(data):\n",
    "        # Convert datetime column\n",
    "        data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])\n",
    "        data['hour'] = data['trans_date_trans_time'].dt.hour\n",
    "        data['day'] = data['trans_date_trans_time'].dt.day\n",
    "        data['month'] = data['trans_date_trans_time'].dt.month\n",
    "        data['year'] = data['trans_date_trans_time'].dt.year\n",
    "        data['day_of_week'] = data['trans_date_trans_time'].dt.dayofweek\n",
    "\n",
    "        # Calculate user age\n",
    "        data['dob'] = pd.to_datetime(data['dob'])\n",
    "        data['age'] = (data['trans_date_trans_time'] - data['dob']).dt.days // 365\n",
    "    \n",
    "        data['distance'] = haversine(data['lat'], data['long'], data['merch_lat'], data['merch_long'])\n",
    "\n",
    "        #Task 1: Generate number of transactions per card in specific time windows\n",
    "        data['trans_date'] = pd.to_datetime(data['trans_date_trans_time']).dt.date\n",
    "        data['trans_hour'] = pd.to_datetime(data['trans_date_trans_time']).dt.hour\n",
    "        data['trans_month'] = pd.to_datetime(data['trans_date_trans_time']).dt.to_period('M')\n",
    "        data['trans_year'] = pd.to_datetime(data['trans_date_trans_time']).dt.year\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def compute_aggregated_metrics(data):\n",
    "        \n",
    "        data = create_category_fraud_probability(data)\n",
    "        data = create_merchant_fraud_probability(data)\n",
    "        data = create_fraud_prob_by_amt(data)\n",
    "        data = create_hourly_fraud_probability(data)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def remove_useless_columns(data):   \n",
    "\n",
    "        # Task 5: Drop specified columns\n",
    "        columns_to_drop = ['trans_date_trans_time', 'first', 'last', 'street', 'city', 'state', \n",
    "                           'lat', 'long', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long',\n",
    "                          'trans_date', 'trans_hour', 'trans_month', 'trans_year', 'cc_num', 'job']\n",
    "\n",
    "        return data.drop(columns=columns_to_drop)\n",
    "    ############################################################################################\n",
    "    \n",
    "    # Ensure the transaction time is in datetime format\n",
    "    complete_df['trans_date_trans_time'] = pd.to_datetime(complete_df['trans_date_trans_time'])\n",
    "\n",
    "    # Sort the dataset by transaction time\n",
    "    complete_df = complete_df.sort_values('trans_date_trans_time').reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Creating new not aggregated metrics\n",
    "    complete_df = not_aggreageted_metric(complete_df)\n",
    "    \n",
    "    \n",
    "    if historical:\n",
    "        # computing and transfer aggregated features\n",
    "        complete_df = compute_aggregated_metrics(complete_df)\n",
    "    \n",
    "    \n",
    "    # Sort the dataset by transaction time ---> VERY IMPORTANT!! Avoid reindexing int the compute_aggregated_metrics\n",
    "    complete_df = complete_df.sort_values('trans_date_trans_time').reset_index(drop=True)\n",
    "    \n",
    "    # dropping useless columns\n",
    "    complete_df = remove_useless_columns(complete_df)\n",
    "    \n",
    "\n",
    "\n",
    "    # Define time-based split ratios\n",
    "    train_split = 0.70  # 60% for training\n",
    "    val_split = 0.15    # 20% for validation\n",
    "    test_split = 0.15   # 20% for testing\n",
    "\n",
    "    # Split the dataset into training, validation, and test sets based on time\n",
    "    train_size = int(len(complete_df) * 0.7)\n",
    "    val_size = int(len(complete_df) * 0.15)\n",
    "\n",
    "    train_data = complete_df[:train_size]\n",
    "    val_data = complete_df[train_size:train_size+val_size]\n",
    "    test_data = complete_df[train_size+val_size:]\n",
    "\n",
    "    # Print the number of rows in each split\n",
    "    print(f\"Training set: {len(train_data)} rows\")\n",
    "    print(f\"Validation set: {len(val_data)} rows\")\n",
    "    print(f\"Test set: {len(test_data)} rows\")\n",
    "    \n",
    "    # Calculate the frequency of fraud for each dataset\n",
    "    def calculate_fraud_frequency(df):\n",
    "        total_transactions = len(df)\n",
    "        total_fraud = df['is_fraud'].sum()\n",
    "        return (total_fraud / total_transactions) * 100\n",
    "\n",
    "    fraud_freq_train = calculate_fraud_frequency(train_data)\n",
    "    fraud_freq_val = calculate_fraud_frequency(val_data)\n",
    "    fraud_freq_test = calculate_fraud_frequency(test_data)\n",
    "    \n",
    "    # Print the fraud frequencies for each dataset\n",
    "    print(f\"Fraud Frequency in Training Data: {fraud_freq_train:.2f}%\")\n",
    "    print(f\"Fraud Frequency in Validation Data: {fraud_freq_val:.2f}%\")\n",
    "    print(f\"Fraud Frequency in Test Data: {fraud_freq_test:.2f}%\")\n",
    "\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def data_processing(data):\n",
    "    # Convert datetime column\n",
    "    data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])\n",
    "    data['hour'] = data['trans_date_trans_time'].dt.hour\n",
    "    data['day'] = data['trans_date_trans_time'].dt.day\n",
    "    data['month'] = data['trans_date_trans_time'].dt.month\n",
    "    data['year'] = data['trans_date_trans_time'].dt.year\n",
    "    data['day_of_week'] = data['trans_date_trans_time'].dt.dayofweek\n",
    "\n",
    "    # Calculate user age\n",
    "    data['dob'] = pd.to_datetime(data['dob'])\n",
    "    data['age'] = (data['trans_date_trans_time'] - data['dob']).dt.days // 365\n",
    "\n",
    "    # Distance between transaction and merchant locations\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        # Radius of Earth in kilometers\n",
    "        R = 6371\n",
    "        # Convert degrees to radians\n",
    "        phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "        dphi = np.radians(lat2 - lat1)\n",
    "        dlambda = np.radians(lon2 - lon1)\n",
    "        a = np.sin(dphi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2)**2\n",
    "        return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    data['distance'] = haversine(data['lat'], data['long'], data['merch_lat'], data['merch_long'])\n",
    "\n",
    "    #Task 1: Generate number of transactions per card in specific time windows\n",
    "    data['trans_date'] = pd.to_datetime(data['trans_date_trans_time']).dt.date\n",
    "    data['trans_hour'] = pd.to_datetime(data['trans_date_trans_time']).dt.hour\n",
    "    data['trans_month'] = pd.to_datetime(data['trans_date_trans_time']).dt.to_period('M')\n",
    "    data['trans_year'] = pd.to_datetime(data['trans_date_trans_time']).dt.year\n",
    "\n",
    "    # Aggregate transactions per hour, day, month, and year\n",
    "    data['transactions_per_hour'] = data.groupby(['cc_num', 'trans_date', 'trans_hour'])['cc_num'].transform('count')\n",
    "    data['transactions_per_day'] = data.groupby(['cc_num', 'trans_date'])['cc_num'].transform('count')\n",
    "    data['transactions_per_month'] = data.groupby(['cc_num', 'trans_month'])['cc_num'].transform('count')\n",
    "    data['transactions_per_year'] = data.groupby(['cc_num', 'trans_year'])['cc_num'].transform('count')\n",
    "    data['avg_amt_cc'] = data.groupby('cc_num')['amt'].transform('mean')\n",
    "\n",
    "    # Task 2: Aggregate statistics for merchant\n",
    "    # Average transaction amount per merchant\n",
    "    data['avg_amt_per_merchant'] = data.groupby('merchant')['amt'].transform('mean')\n",
    "\n",
    "    # Fraud ratio per merchant\n",
    "    data['fraud_ratio_per_merchant'] = data.groupby('merchant')['is_fraud'].transform('mean')\n",
    "\n",
    "    # Task 3: Replace category with fraud likelihood per category\n",
    "    data['fraud_likelihood_per_category'] = data.groupby('category')['is_fraud'].transform('mean')\n",
    "    \n",
    "    # 1. Compute ratio_1: Number of fraud related to a bank / Number of total fraud\n",
    "    # First, create a fraud-related bank count and total fraud count\n",
    "    fraud_by_bank = data[data['is_fraud'] == 1].groupby('merchant').size()\n",
    "    total_fraud = len(data[data['is_fraud'] == 1])\n",
    "\n",
    "    # Merge fraud count by bank with the original dataframe\n",
    "    data['fraud_count_bank'] = data['merchant'].map(fraud_by_bank)\n",
    "    data['ratio_1'] = (data['fraud_count_bank'] / total_fraud).fillna(0)\n",
    "\n",
    "    # 2. Compute ratio_2: Number of fraud related to a bank / Number of transactions of the bank\n",
    "    # Count total transactions for each bank\n",
    "    transactions_by_bank = data.groupby('merchant').size()\n",
    "\n",
    "    # Merge transaction count by bank with the original dataframe\n",
    "    data['total_transactions_bank'] = data['merchant'].map(transactions_by_bank)\n",
    "    data['ratio_2'] = (data['fraud_count_bank'] / data['total_transactions_bank']).fillna(0)\n",
    "\n",
    "    # 3. Compute total_amt_cc: Total amount spent by the same credit card\n",
    "    total_amt_cc = data.groupby('cc_num')['amt'].sum()\n",
    "\n",
    "    # Merge total amount by cc_num with the original dataframe\n",
    "    data['total_amt_cc'] = data['cc_num'].map(total_amt_cc)\n",
    "\n",
    "    # 4. Compute number_trans_cc: Total number of transactions by the same credit card\n",
    "    number_trans_cc = data.groupby('cc_num').size()\n",
    "\n",
    "    # Merge transaction count by cc_num with the original dataframe\n",
    "    data['number_trans_cc'] = data['cc_num'].map(number_trans_cc)\n",
    "\n",
    "    # Task 5: Drop specified columns\n",
    "    columns_to_drop = ['trans_date_trans_time', 'first', 'last', 'street', 'city', 'state', \n",
    "                       'lat', 'long', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long',\n",
    "                      'trans_date', 'trans_hour', 'trans_month', 'trans_year', 'cc_num', 'fraud_count_bank', \n",
    "                       'total_transactions_bank' ]\n",
    "\n",
    "    data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def encode_and_scale(train_data, validation_data, test_data, historical=False):\n",
    "    \n",
    "    print(\"removing jobs missing in train dataset but existing in test dataset\")\n",
    "    # removing jobs missing in train dataset but existing in test dataset\n",
    "    #missing_jobs = ['Software engineer', 'Operational investment banker', 'Engineer, water']\n",
    "\n",
    "    #for job in missing_jobs:\n",
    "    #    test_data = test_data[test_data['job'] != job]\n",
    "    #    validation_data = validation_data[validation_data['job'] != job]  # Same for validation dataset\n",
    "\n",
    "    print(\"Encode gender\")\n",
    "    # Encode gender\n",
    "    train_data['gender'] = train_data['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "    validation_data['gender'] = validation_data['gender'].apply(lambda x: 1 if x == 'M' else 0)  # Apply to validation\n",
    "    test_data['gender'] = test_data['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "\n",
    "    categorical_cols = ['merchant', 'category']\n",
    "    label_encoders = {}\n",
    "    \n",
    "    print(\"Label Encoding\")\n",
    "    for col in categorical_cols:\n",
    "        print(col)\n",
    "        # Create a LabelEncoder instance\n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        print(\"Fit on training data and get classes\")\n",
    "        # Fit on training data and get classes\n",
    "        train_classes = train_data[col].unique()\n",
    "        le.fit(train_classes)\n",
    "        \n",
    "        print(\"Transform train_data\")\n",
    "        # Transform train_data\n",
    "        train_data[col] = le.transform(train_data[col])\n",
    "        \n",
    "        print(\"Transform validation_data\")\n",
    "        # Transform validation_data\n",
    "        validation_data[col] = le.transform(validation_data[col])\n",
    "        \n",
    "        print(\"Transform test_data\")\n",
    "        # Transform test_data\n",
    "        test_data[col] = le.transform(test_data[col])\n",
    "\n",
    "        # Save the LabelEncoder for future use\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    print(\"Scale continuous variables\")\n",
    "    # Scale continuous variables\n",
    "    if historical:\n",
    "        continuous_cols = ['amt', 'city_pop', 'hour', 'day', 'month', 'year', 'day_of_week', \n",
    "                   'age', 'distance', 'category_fraud_probability', 'merchant_fraud_probability', \n",
    "                    'fraud_prob_by_amt', 'hourly_fraud_probability']\n",
    "    else:\n",
    "        continuous_cols = ['amt', 'city_pop', 'hour', 'day', 'month', 'year', 'day_of_week', \n",
    "                   'age', 'distance']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    print(\"fit_transform train_data\")\n",
    "    train_data[continuous_cols] = scaler.fit_transform(train_data[continuous_cols])\n",
    "    print(\"transform validation_data\")\n",
    "    validation_data[continuous_cols] = scaler.transform(validation_data[continuous_cols])  # Scale validation data\n",
    "    print(\"transform test_data\")\n",
    "    test_data[continuous_cols] = scaler.transform(test_data[continuous_cols])\n",
    "\n",
    "    return train_data, validation_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5825f33e",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47168146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\s.crispino\\AppData\\Local\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Custom Focal Loss function\n",
    "def focal_loss(alpha=[0.25, 0.75], gamma=2.0):\n",
    "    print(\"Custom Focal Loss Params\")\n",
    "    print(\"alpha: \", alpha)\n",
    "    print(\"gamma: \", gamma)\n",
    "    \"\"\"\n",
    "    Focal Loss for binary classification with class-specific alpha.\n",
    "    \n",
    "    Parameters:\n",
    "    - alpha: List with two values: [alpha for negative class (0), alpha for positive class (1)].\n",
    "    - gamma: Focusing parameter.\n",
    "    \n",
    "    Returns:\n",
    "    - Loss function to be used in model.compile().\n",
    "    \"\"\"\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Cast y_true to float32 to match y_pred\n",
    "        ### Ensures y_true is in float32 format for correct tensor operation\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        # Clip predictions to avoid log(0)\n",
    "        ### Clips the predicted probability (y_pred) to a small epsilon value (K.epsilon() ≈ 10^{-7}) \n",
    "        ### to prevent numerical instability when computing logarithms.\n",
    "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "        # Compute alpha dynamically based on class (element-wise multiplication)\n",
    "         # IF y_true = 1 => alpha_t = y_true * alpha[1]\n",
    "         # IF y_true = 0 => alpha_t = (1 - y_true) * alpha[0]\n",
    "        alpha_t = y_true * alpha[1] + (1 - y_true) * alpha[0]\n",
    "\n",
    "        # Calculate standard binary cross-entropy (BCE)\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        \n",
    "        # Compute focal loss with class-specific alpha\n",
    "            # tf.math.pow(1 - y_pred, gamma)\n",
    "                # Applying a  power to the error (1-y_pred) it Down-weights well-classified examples \n",
    "                    # (small errors became smaller)\n",
    "                    # Consider y_pred = P(is_fraud = 1)\n",
    "                # Es:\n",
    "                    # IF y_true = 1 and y_pred is close to 1 (1-y_pred) = small error became smaller\n",
    "                    # IF y_true = 1 and y_pred is close to 0 (1-y_pred) = big error became bigger\n",
    "            # alpha_t *\n",
    "                # weights the loss result giving more importance to fraud class y_true = 1\n",
    "        focal_loss = alpha_t * tf.math.pow(1 - y_pred, gamma) * cross_entropy\n",
    "        \n",
    "        return tf.reduce_mean(focal_loss)\n",
    "\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "\n",
    "def create_FFN(input_dim, num_blocks=3, initial_units=128, dropout_rate=0.3, activation='relu', \n",
    "               l1_reg=0.0, l2_reg=0.0, custom_focal_loss=False, alpha=0.25, gamma=2.0, \n",
    "               learning_rate=0.001, reduction_rate=0.5):\n",
    "    \"\"\"\n",
    "    Create a dynamic Feedforward Neural Network with L1/L2 regularization, customizable layers, and other parameters.\n",
    "\n",
    "    Parameters:\n",
    "        input_dim (int): Input dimension for the first layer.\n",
    "        num_blocks (int): Number of Dense-BatchNorm-Dropout blocks.\n",
    "        initial_units (int): Number of units in the first Dense layer. Each subsequent block halves this number.\n",
    "        dropout_rate (float): Dropout rate for Dropout layers.\n",
    "        activation (str): Activation function for Dense layers (except the output layer).\n",
    "        l1_reg (float): L1 regularization coefficient.\n",
    "        l2_reg (float): L2 regularization coefficient.\n",
    "        custom_focal_loss (bool): Whether to use a custom focal loss.\n",
    "        alpha (float): Alpha parameter for focal loss.\n",
    "        gamma (float): Gamma parameter for focal loss.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        model (tf.keras.Model): Compiled Keras model.\n",
    "    \"\"\"\n",
    "    print(\"Model building...\")\n",
    "\n",
    "    # Hyperparameters dictionary for reference\n",
    "    hyperparameters = {\n",
    "        \"num_blocks\": num_blocks,\n",
    "        \"initial_units\": initial_units,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"activation\": activation,\n",
    "        \"l1_reg\": l1_reg,\n",
    "        \"l2_reg\": l2_reg,\n",
    "        \"custom_focal_loss\": custom_focal_loss,\n",
    "        \"alpha\": alpha,\n",
    "        \"gamma\": gamma,\n",
    "    }\n",
    "\n",
    "    # Create a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the first Dense layer with L1/L2 regularization\n",
    "    units = initial_units\n",
    "    model.add(Dense(units, input_dim=input_dim, activation=activation,\n",
    "                    kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Add subsequent Dense-BatchNorm-Dropout blocks\n",
    "    for layer_number in range(num_blocks - 1):\n",
    "        units = max(1, int(units*reduction_rate))  # Ensure units do not go below 1\n",
    "        model.add(Dense(units, activation=activation,\n",
    "                        kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
    "        if layer_number < num_blocks - 2:\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Add the output layer for binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    if custom_focal_loss:\n",
    "        print(\"Warning: Using a custom focal loss\")\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss=focal_loss(alpha, gamma),\n",
    "            metrics=['accuracy', \n",
    "                     AUC(curve='PR', name='pr_auc')  # Precision-Recall AUC  <-- #tf.keras.metrics.AUC(name='auc')\n",
    "                    ]\n",
    "        )\n",
    "    else:\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', \n",
    "                     AUC(curve='PR', name='pr_auc')  # Precision-Recall AUC  <-- #tf.keras.metrics.AUC(name='auc')\n",
    "                    ]\n",
    "        )\n",
    "\n",
    "    # Print model summary\n",
    "    print(model.summary())\n",
    "\n",
    "    return model, hyperparameters\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.layers import Input, Dense, BatchNormalization, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, LeakyReLU, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_autoencoder(X_train, encoding_dim=6, reduction_rate=0.5, activation='LeakyReLU', dropout_rate=0.2, learning_rate=0.0001, l1=0.01, l2=0.01):\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = input_layer\n",
    "\n",
    "    # Encoder layers (store dimensions for symmetry)\n",
    "    encoder_dims = []\n",
    "    current_dim = input_dim\n",
    "    while current_dim > encoding_dim:\n",
    "        current_dim = max(encoding_dim, int(current_dim * reduction_rate))\n",
    "        encoder_dims.append(current_dim)  # Store dimension\n",
    "        encoded = Dense(current_dim, activation=None, kernel_regularizer=l1_l2(l1=l1, l2=l2))(encoded)\n",
    "        encoded = BatchNormalization()(encoded)\n",
    "        encoded = LeakyReLU(alpha=0.1)(encoded)\n",
    "        encoded = Dropout(dropout_rate)(encoded)\n",
    "\n",
    "    # Bottleneck (optional activation)\n",
    "    # encoded = Activation('linear')(encoded)  # Or no activation at all\n",
    "\n",
    "    # Decoder layers (reverse encoder dimensions)\n",
    "    decoded = encoded\n",
    "    for current_dim in reversed(encoder_dims):  # Reverse the dimensions\n",
    "        decoded = Dense(current_dim, activation=None, kernel_regularizer=l1_l2(l1=l1, l2=l2))(decoded)\n",
    "        decoded = BatchNormalization()(decoded)\n",
    "        decoded = LeakyReLU(alpha=0.1)(decoded)\n",
    "\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)  # Linear activation for output\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "    print(autoencoder.summary())\n",
    "    return autoencoder, {  # Return hyperparameters as a dictionary\n",
    "        \"encoding_dim\": encoding_dim,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"activation\": activation,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"l1\": l1,\n",
    "        \"l2\": l2\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick: Sample latent space using mean and log variance.\"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=K.shape(z_mean), mean=0., stddev=1.0)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def create_variational_autoencoder(X_train, encoding_dim=6, dropout_rate=0.2, learning_rate=0.0001, l1=0.01, l2=0.01):\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Dense(64, activation=None, kernel_regularizer=l1_l2(l1=l1, l2=l2))(input_layer)  # Apply L1_L2 regularization\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = LeakyReLU(alpha=0.1)(encoded)\n",
    "    encoded = Dense(32, activation=None, kernel_regularizer=l1_l2(l1=l1, l2=l2))(encoded)  # Apply L1_L2 regularization\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = LeakyReLU(alpha=0.1)(encoded)\n",
    "    \n",
    "    # Latent space\n",
    "    z_mean = Dense(encoding_dim, name=\"z_mean\")(encoded)\n",
    "    z_log_var = Dense(encoding_dim, name=\"z_log_var\")(encoded)\n",
    "    z = Lambda(sampling, output_shape=(encoding_dim,), name=\"z\")([z_mean, z_log_var])\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = Dense(32, activation=None, kernel_regularizer=l1_l2(l1=l1, l2=l2))(z)  # Apply L1_L2 regularization\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = LeakyReLU(alpha=0.1)(decoded)\n",
    "    decoded = Dense(64, activation=None, kernel_regularizer=l1_l2(l1=l1, l2=l2))(decoded)  # Apply L1_L2 regularization\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = LeakyReLU(alpha=0.1)(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    # VAE model\n",
    "    vae = Model(inputs=input_layer, outputs=decoded)\n",
    "    \n",
    "    # Define VAE loss\n",
    "    reconstruction_loss = mse(input_layer, decoded)\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer=Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return vae, Model(inputs=input_layer, outputs=[z_mean, z_log_var])  # Return the VAE and encoder (for latent representations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90cd4a8",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306f6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(history):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))  # Three subplots for Loss, Accuracy, and PR AUC\n",
    "    fig.tight_layout(pad=4.0)\n",
    "\n",
    "    # Extract metrics\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    if 'accuracy' in history.history:  # If accuracy is being tracked\n",
    "        train_acc = history.history['accuracy']\n",
    "        val_acc = history.history['val_accuracy']\n",
    "\n",
    "        # Plot Loss\n",
    "        ax[0].set_xlabel('Epochs')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].set_title('Loss')\n",
    "        ax[0].plot(train_loss, label='Training Loss')\n",
    "        ax[0].plot(val_loss, label='Validation Loss')\n",
    "        ax[0].legend()\n",
    "\n",
    "        # Plot Accuracy\n",
    "        ax[1].set_xlabel('Epochs')\n",
    "        ax[1].set_ylabel('Accuracy')\n",
    "        ax[1].set_title('Accuracy')\n",
    "        ax[1].plot(train_acc, label='Training Accuracy')\n",
    "        ax[1].plot(val_acc, label='Validation Accuracy')\n",
    "        ax[1].legend()\n",
    "    else:\n",
    "        # Plot Loss only\n",
    "        ax[0].set_xlabel('Epochs')\n",
    "        ax[0].set_ylabel('Loss')\n",
    "        ax[0].set_title('Loss')\n",
    "        ax[0].plot(train_loss, label='Training Loss')\n",
    "        ax[0].plot(val_loss, label='Validation Loss')\n",
    "        ax[0].legend()\n",
    "        ax[1].remove()  # Remove the unused subplot\n",
    "\n",
    "    # Check if PR AUC exists\n",
    "    if 'pr_auc' in history.history:\n",
    "        train_pr_auc = history.history['pr_auc']\n",
    "        val_pr_auc = history.history['val_pr_auc']\n",
    "\n",
    "        # Plot Precision-Recall AUC\n",
    "        ax[2].set_xlabel('Epochs')\n",
    "        ax[2].set_ylabel('PR AUC')\n",
    "        ax[2].set_title('Precision-Recall AUC')\n",
    "        ax[2].plot(train_pr_auc, label='Training PR AUC')\n",
    "        ax[2].plot(val_pr_auc, label='Validation PR AUC')\n",
    "        ax[2].legend()\n",
    "    else:\n",
    "        ax[2].remove()  # Remove the unused subplot if no PR AUC\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm):\n",
    "\n",
    "    # Plotting confusion matrix using seaborn's heatmap\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False, xticklabels=['Non-Fraud', 'Fraud'], yticklabels=['Non-Fraud', 'Fraud'])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "from sklearn.metrics import average_precision_score    \n",
    "\n",
    "def compute_main_scores(y_test, y_pred, y_pred_prob):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Compute AUC\n",
    "    pr_auc = average_precision_score(y_test, y_pred_prob)\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision-Recall AUC Score: {pr_auc:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    plot_confusion_matrix(cm)\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    \n",
    "    return accuracy, pr_auc, recall, f1, precision\n",
    "\n",
    "    \n",
    "def evaluation_on_test(model, X_test, y_test, best_threshold=0.5):\n",
    "\n",
    "    # Predictions and classification report\n",
    "    y_pred_prob = model.predict(X_test).flatten()\n",
    "    y_pred = (y_pred_prob >= best_threshold).astype(int)\n",
    "    \n",
    "    compute_main_scores(y_test, y_pred, y_pred_prob)\n",
    "    \n",
    "    \n",
    "from sklearn.metrics import precision_recall_curve, f1_score, roc_auc_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "def threshold_metric_evaluation(model, X_val, y_val):\n",
    "    \n",
    "    # Get probabilities on validation data\n",
    "    y_pred_prob = model.predict(X_val).flatten()\n",
    "    \n",
    "    # Define thresholds\n",
    "    thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Evaluate metrics for each threshold\n",
    "    for thresh in thresholds:\n",
    "        y_pred_thresh = (y_pred_prob >= thresh).astype(int)\n",
    "        precision = precision_score(y_val, y_pred_thresh, zero_division=0)\n",
    "        recall = recall_score(y_val, y_pred_thresh)\n",
    "        f1 = f1_score(y_val, y_pred_thresh)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Filter thresholds that satisfy recall >= 0.7\n",
    "    valid_indices = [i for i, recall in enumerate(recall_scores) if recall >= 0.7]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        print(\"No thresholds satisfy the recall constraint of 0.7.\")\n",
    "        return\n",
    "\n",
    "    # Among valid thresholds, find the one with the best F1-Score\n",
    "    valid_f1_scores = [f1_scores[i] for i in valid_indices]\n",
    "    best_threshold_idx = valid_indices[np.argmax(valid_f1_scores)]\n",
    "    best_threshold = thresholds[best_threshold_idx]\n",
    "    \n",
    "    print(f\"Best Threshold (Recall >= 0.7 & Max F1-Score): {best_threshold:.2f}\")\n",
    "    print(f\"Precision: {precision_scores[best_threshold_idx]:.2f}, Recall: {recall_scores[best_threshold_idx]:.2f}, F1-Score: {f1_scores[best_threshold_idx]:.2f}\")\n",
    "    \n",
    "    # Plot the metrics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, precision_scores, label=\"Precision\", marker='o')\n",
    "    plt.plot(thresholds, recall_scores, label=\"Recall\", marker='o')\n",
    "    plt.plot(thresholds, f1_scores, label=\"F1-Score\", marker='o')\n",
    "    plt.axvline(best_threshold, color='r', linestyle='--', label=f\"Best Threshold ({best_threshold:.2f})\")\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.ylabel(\"Metric Value\")\n",
    "    plt.title(\"Precision, Recall, and F1-Score vs. Decision Threshold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "def get_experiments_log():\n",
    "    csv_path = \"experiment_log.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def update_experiment_log(csv_path, experiment_data):\n",
    "    \"\"\"\n",
    "    Update the experiment log with new experiment data and save it to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        csv_path (str): The file path to the CSV file.\n",
    "        experiment_data (dict): A dictionary containing the experiment details:\n",
    "            - timestamp (str): The experiment timestamp.\n",
    "            - model_type (str): The type of model used.\n",
    "            - note (str): A personal note about the experiment.\n",
    "            - hyperparameters (dict): A dictionary of hyperparameters used.\n",
    "            - excluded_features (list): The set of feature names removed for the experiment.\n",
    "            - best_threshold (float): The best threshold used for classification.\n",
    "            - val_precision (float): The validation precision metric.\n",
    "            - val_recall (float): The validation recall metric.\n",
    "            - val_f1_score (float): The validation F1-score.\n",
    "            - val_accuracy (float): The validation accuracy metric.\n",
    "            - val_auc (float): The validation AUC metric.\n",
    "            - val_aic (float): The validation AIC value.\n",
    "            - val_smd (float): The validation SMD value.\n",
    "            - test_precision (float): The test precision metric.\n",
    "            - test_recall (float): The test recall metric.\n",
    "            - test_f1_score (float): The test F1-score.\n",
    "            - test_accuracy (float): The test accuracy metric.\n",
    "            - test_auc (float): The test AUC metric.\n",
    "            - test_aic (float): The test AIC value.\n",
    "            - test_smd (float): The test SMD value.\n",
    "    \"\"\"\n",
    "    # Convert the dictionary of hyperparameters to a JSON-like string\n",
    "    experiment_data[\"hyperparameters\"] = str(experiment_data[\"hyperparameters\"])\n",
    "\n",
    "    # Convert the list of features to a comma-separated string\n",
    "    experiment_data[\"excluded_features\"] = \"- \".join(experiment_data[\"excluded_features\"])\n",
    "\n",
    "    # Check if the CSV file exists\n",
    "    if os.path.exists(csv_path):\n",
    "        # Load the existing DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        # Create a new DataFrame\n",
    "        columns = [\n",
    "            \"timestamp\", \"model_type\", \"tuning_on\", \"features_note\", \"dataset_size\", \"note\", \"hyperparameters\", \n",
    "            \"excluded_features\", \"best_threshold\",\n",
    "            \"val_precision\", \"val_recall\", \"val_f1\", \"val_accuracy\", \"val_auc\", \"val_aic\", \"val_smd\",\n",
    "            \"test_precision\", \"test_recall\", \"test_f1\", \"test_accuracy\", \"test_auc\", \"test_aic\", \"test_smd\",\n",
    "    \n",
    "        ]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Append the new data to the DataFrame\n",
    "    df = pd.concat([df, pd.DataFrame([experiment_data])], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame to the CSV file\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    \n",
    "def save_experiment_data(experiment_data, csv_path):\n",
    "    \"\"\"\n",
    "    Save experiment data to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        experiment_data (dict): A dictionary containing the experiment data.\n",
    "        file_name (str): The name of the CSV file to save the data.\n",
    "    \"\"\"\n",
    "    # Define the header based on the keys of the dictionary\n",
    "    header = list(experiment_data.keys())\n",
    "\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "\n",
    "    # Open the file in append mode\n",
    "    with open(csv_path, mode='a', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "\n",
    "        # Write the header only if the file does not exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Write the experiment data\n",
    "        writer.writerow(experiment_data)\n",
    "\n",
    "    print(f\"Experiment data saved to {csv_path}\")\n",
    "\n",
    "    \n",
    "\n",
    "def compare_test_performance(csv):\n",
    "    # Load the dataset\n",
    "    performance_df = pd.read_csv(csv)\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Create a single figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(12, 4), sharey=True)\n",
    "    \n",
    "    # Plot Test F1 vs Tuning On\n",
    "    sns.barplot(ax=axes[0], x='test_accuracy', y='model_type', data=performance_df, palette='viridis')\n",
    "    axes[0].set_xlabel('Test Accuracies')\n",
    "    axes[0].set_ylabel('')  # No y-label for the second plot\n",
    "    axes[0].set_title('Test Accuracies')\n",
    "    axes[0].grid(alpha=0.25)\n",
    "\n",
    "    # Plot Test Recall vs Tuning On\n",
    "    sns.barplot(ax=axes[1], x='test_recall', y='model_type', data=performance_df, palette='viridis')\n",
    "    axes[1].set_xlabel('Test Recall')\n",
    "    axes[1].set_ylabel('')\n",
    "    axes[1].set_title('Test Recall')\n",
    "    axes[1].grid(alpha=0.25)\n",
    "\n",
    "    # Plot Test F1 vs Tuning On\n",
    "    sns.barplot(ax=axes[2], x='test_f1', y='model_type', data=performance_df, palette='viridis')\n",
    "    axes[2].set_xlabel('Test F1')\n",
    "    axes[2].set_ylabel('')  # No y-label for the second plot\n",
    "    axes[2].set_title('Test F1')\n",
    "    axes[2].grid(alpha=0.25)\n",
    "    \n",
    "    # Plot Test F1 vs Tuning On\n",
    "    sns.barplot(ax=axes[3], x='test_auc', y='model_type', data=performance_df, palette='viridis')\n",
    "    axes[3].set_xlabel('Test PR AUC')\n",
    "    axes[3].set_ylabel('')  # No y-label for the second plot\n",
    "    axes[3].set_title('Test PR AUC')\n",
    "    axes[3].grid(alpha=0.25)\n",
    "    \n",
    "    # test_aic\n",
    "    sns.barplot(ax=axes[4], x='test_aic', y='model_type', data=performance_df, palette='viridis')\n",
    "    axes[4].set_xlabel('Test AIC')\n",
    "    axes[4].set_ylabel('')  # No y-label for the second plot\n",
    "    axes[4].set_title('Test AIC')\n",
    "    axes[4].grid(alpha=0.25)\n",
    "    \n",
    "\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define a function to save training information\n",
    "def save_validation_info(validation_performance, csv_file):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    \n",
    "    # Open the file in append mode\n",
    "    with open(csv_file, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=validation_performance.keys())\n",
    "        \n",
    "        # Write the header only if the file is new\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Write the current training information\n",
    "        writer.writerow(validation_performance)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d55d8f",
   "metadata": {},
   "source": [
    "## Fairness Evaluation Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "359e2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adverse_impacat_ratio(df, marker, val, control_val, target, target_val):\n",
    "    \n",
    "    control_pop = df[df[marker]==control_val]\n",
    "    control_number = control_pop[marker].count()\n",
    "    control_pos_target = control_pop[df[target]==target_val][marker].count()\n",
    "    \n",
    "    control_correct_target = control_pop[df[target]==df['label']][marker].count()\n",
    "    \n",
    "    # ratios\n",
    "    control_pos_rate = control_pos_target / control_number\n",
    "    control_accuracy = control_correct_target / control_number\n",
    "    \n",
    "    \n",
    "    marker_pop = df[df[marker]==val]\n",
    "    pop_number = marker_pop[marker].count()\n",
    "    pop_pos_target = marker_pop[df[target]==target_val][marker].count()\n",
    "\n",
    "    pop_correct_target = marker_pop[df[target]==df['label']][marker].count()\n",
    "\n",
    "    #ratios\n",
    "    pop_pos_rate = pop_pos_target / pop_number\n",
    "    pop_accuracy = pop_correct_target / pop_number\n",
    "\n",
    "    AIR_outcome = pop_pos_rate / control_pos_rate\n",
    "    # Unfair in accuracy metric\n",
    "    AIR_accuracy = pop_accuracy / control_accuracy\n",
    "\n",
    "        \n",
    "    return AIR_outcome, AIR_accuracy\n",
    "    \n",
    "    \n",
    "    \n",
    "def standardized_mean_difference(df, marker, val, control_val, target):\n",
    "    # small 0.2, 0.5, and 0.8 large\n",
    "\n",
    "    control_values = df[df[marker] == control_val][target].values\n",
    "    control_mean = np.mean(control_values)\n",
    "    control_std = np.std(control_values)\n",
    "\n",
    "\n",
    "    pop_values = df[df[marker] == val][target].values\n",
    "    pop_mean = np.mean(pop_values)\n",
    "    pop_std = np.std(pop_values)\n",
    "\n",
    "    smd = (pop_mean - control_mean) / np.sqrt((pop_std**2 + control_std**2) / 2)\n",
    "\n",
    "    return smd\n",
    "\n",
    "\n",
    "\n",
    "def get_final_result_df(labels, predictions, X_test):\n",
    "    \n",
    "    print(labels.shape)\n",
    "    print(predictions.shape)\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    results_df = pd.DataFrame({'label': labels, 'prediction': predictions})\n",
    "\n",
    "    # Concatenate the original test data with the results DataFrame\n",
    "    final_results_df = pd.concat([X_test, results_df], axis=1)\n",
    "\n",
    "    return final_results_df\n",
    "\n",
    "\n",
    "\n",
    "####### FAIRNESS METRIC EVALUATION:\n",
    "# es: demographich_marker=\"gender\", pop=0, control_pop= 1, pos_outcome=0\n",
    "def fairness_evaluation(model, X_val, y_val, y_val_pred_classes, demographich_marker, pop, control_pop, pos_outcome):\n",
    "\n",
    "    conc_result_df = get_final_result_df(y_val, y_val_pred_classes.ravel(), X_val)                        \n",
    "    smd = standardized_mean_difference(conc_result_df, demographich_marker, 0, 1, 'prediction')\n",
    "    air_outcome, air_out_accuracy = adverse_impacat_ratio(conc_result_df, demographich_marker, 0, 1, 'prediction', 0)\n",
    "    \n",
    "    print(f\"Standardized mean difference of non-fraud {demographich_marker}: \", smd)\n",
    "    print(f\"Adverse Impact Ratio of non-fraud for {demographich_marker}: \", air_outcome)\n",
    "    print(f\"Adverse Impact Ratio of accuracy for {demographich_marker}: \", air_out_accuracy)\n",
    "    \n",
    "    return conc_result_df, smd, air_outcome, air_out_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def create_biased_df(df,  biased_fraction):\n",
    "\n",
    "    biased_df = df.copy()\n",
    "\n",
    "    # make it deterministic\n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "    female_shape = biased_df[biased_df['sex'] == 0].shape[0]\n",
    "   \n",
    "    biased_indices = np.random.choice(biased_df[biased_df['sex'] == 0][biased_df['credit_risk'] != 0].index, \n",
    "                                      size=int(biased_fraction * female_shape), replace=False)\n",
    "    biased_df.loc[biased_indices, 'credit_risk'] = 0  \n",
    "\n",
    "    air_score = adverse_impact_ratio_for_df(biased_df, 'sex', 0, 'credit_risk')\n",
    "    print(f\"Adverse Impact Ratio (AIR): {air_score}\")\n",
    "    \n",
    "    sme_score = standardized_mean_difference_for_df(biased_df, 'sex', 0, 1, 'credit_risk')\n",
    "    print(f\"Standardized Mean Difference {sme_score}\")\n",
    "    \n",
    "    return biased_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
